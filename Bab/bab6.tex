\chapter{Kesimpulan dan Saran}

\section{Kesimpulan}

Hadoop merupakan platform \textit{open source} yang cocok untuk melakukan proses terhadap data yang berukuran besar, memiliki skalabilitas yang baik, dan juga memiliki mekanisme penanganan error(\textit{fault tolerance}) yang bagus dapat memberikan solusi bagi industri untuk mengambil informasi dari \textit{big data} dengan waktu yang cukup singkat. Model dari pemrograman \textit{MapReduce} juga cukup mudah dimengerti dan diimplementasikan, dengan pemahaman yang tidak terlalu mendalam mengenai detil dari desain internal pada Hadoop, orang sudah dapat membuat program \textit{MapReduce}. Dengan adanya Hadoop streaming, saat ini program \textit{MapReduce} sudah dapat dibuat menggunakan bahasa lain seperti dengan skrip Python dsb. Hal ini dapat cukup menghilangkan batasan bahasa pemrograman dalam membuat program berbasis \textit{MapReduce} untuk dapat dieksekusi pada lingkungan Hadoop.

Algoritma klasifkasi \textit{naive bayes} merupakan salah satu algoritma dalam teknik penambangan data dan \textit{machine learning} yang dapat sangat bermanfaat pada era \textit{big data} seperti saat ini. Pada era \textit{big data}, teknik penambangan data dapat digunakan oleh perusahaan besar dalam mengubah data yang sudah tidak berguna menjadi sebuah informasi yang sangat berharga, sehingga dapat membantu perusahaan dalam membuat suatu keputusan.

Pada penelitian ini telah berhasil dibangun perangkat lunak yang terdiri dari beberapa modul yang menerapkan algoritma klasifikasi \textit{naive bayes} berbasis \textit{MapReduce} untuk dapat berjalan pada sistem terdistribusi Hadoop. Terdapat 2 modul yang dibuat untuk pembuatan model klasifikasi \textit{naive bayes} dan melakukan evaluasi terhadap model \textit{naive bayes} yang sudah dibuat. Sedangkan 2 modul lainnya dibuat dengan berbasiskan web untuk melakukan pengelolaan input file dalam HDFS dan klasifikasi untuk tiap kasus yang di-input-kan oleh user secara manual.

Sebelum \textit{big data} diumpankan ke dalam algoritma klasifikasi \textit{naive bayes}, big data sebuah studi kasus mengenai pembunuhan yang terjadi di US terlebih dahulu disimpan ke dalam HDFS menggunakan modul kelola input yang telah dibuat dan dilakukan pemilihan atribut kelas dan prediktor yang akan digunakan pada data tersebut. Setelah itu, proses \textit{training} pada pembuatan model klasifikasi \textit{naive bayes} dapat dilakukan dengan pertama - tama membuat tabel frekuensi untuk setiap kemunculan atribut prediktor terhadap atribut kelas tertentu dan untuk atribut bertipe numerik akan melakukan perhitungan rata - rata seluruh atribut tersebut berdasarkan atribut kelas yang lalu diteruskan dengan menghitung standar deviasi untuk digunakan pada perhitungan distribusi normal atribut prediktor numerik terhadap atribut kelas. Akan dilakukan proses evaluasi pada model NBC yang sudah jadi dengan proses yang terpisah dengan proses training untuk melakukan evaluasi terhadap model NBC yang sudah dibuat. Proses evaluasi dan klasifikasi tidak saling bergantung, karena dua - duanya hanya akan menggunaka model NBC yang sudah dibuat sebelumnya.

Berdasarkan hasil pengujian perangkat lunak dan eksperimen yang telah dilakukan pada Bab~\ref{chap:Implementasi, Pengujian, dan Eksperimen}, dapat disimpulkan bahwa :

\begin{itemize}
	\item Perangkat lunak untuk menerapkan algoritma klasifikasi \textit{naive bayes} yang dibuat berbasiskan \textit{MapReduce} pada lingkungan terdistribusi Hadoop telah diuji kebenaran-nya dengan melakukan perbandingan terhadap perhitungan manual pada bagian~\ref{sec:Pengujian Kebenaran} dan menghasilkan nilai yang cukup sama. Perbedaan hanya terjadi pada pembulatan bilangan desimal pada angka dibelakang koma.
	\item Algoritma klasifikasi \textit{naive bayes} pada sistem terdistribusi Hadoop dapat menangani data yang termasuk ke dalam golongan \textit{big data}.
	\item Eksperimen yang telah dilakukan untuk menguji efisiensi dan kecepatan perangkat lunak pada variabel - variabel yang berubah. Berikut ini adalah variabel-variabel beserta kesimpulan hasil eksperimen :
	\begin{enumerate}[label=(\alph*)]
		\item Ukuran blok HDFS\\
		Pada data berukuran 1GB, Semakin besar ukuran blok HDFS untuk file input maka waktu yang dihabiskan untuk menjalankan proses perangkat lunak berbasis \textit{MapReduce} semakin sedikit. Tetapi, perlu diperhatikan bahwa pemilihan ukuran blok file input akan sangat berpengaruh terhadap besaran ukuran file input itu sendiri. Karena, jika ukuran blok HDFS lebih besar atau sama dengan ukuran file itu sendiri, maka file tidak akan didistribusikan kepada tiap \textit{datanode}.
		
		\item Ukuran data\\
		Ukuran data mempengaruhi waktu eksekusi perangkat lunak. Meskipun perbedaan waktu terjadi mungkin karena performa komputer yang jika mengolah data lebih banyak akan memanas dan sedikit melambat.
		
		\item Jumlah Atribut\\
		Jumlah atribut mempengaruhi waktu eksekusi perangkat lunak. Hal tersebut dikarenakan algoritma klasifikasi \textit{naive bayes} melakukan perhitungan jumlah frekuensi untuk tiap nilai atribut prediktor terhadap tiap nilai atribut kelas.
		
		\item Tipe Atribut Prediktor\\
		Perbedaan waktu eksekusi data yg memiliki atribut numerik dibandingkan hanya memiliki atribut diskret memiliki perbedaan yang sangat signifikan. Untuk data yang memiliki atribut numerik memiliki waktu eksekusi yang jauh lebih lama dibandingkan jika seluruh datanya hanya berisi atribut diskrit. Hal tersebut disebabkan karena perhitungan komputasi yang melibatkan atribut numerik memiliki kompleksitas \verb|O(2n)| yang menyebabkan waktu eksekusi akan 2 kali lipat lebih lama daripada atribut diskrit dan melakukan penulisan ke dalam HDFS (untuk output) untuk setiap iterasi.%between marking
		Perbedaan ukuran data yang dapat ditangani oleh perangkat lunak dengan spesifikasi yang sudah dijelaskan jika semua atribut berupa diskret dengan yang memiliki atribut numerik terlihat sangat jauh berbeda. Untuk data yang memiliki atribut numerik hanya mampu memproses hingga ukurannya mencapai 1,11GB saja, selebihnya akan terkena limit pada memori \textit{node} pekerja yang terbatas. Hal tersebut terjadi karena untuk atribut numerik akan melakukan penyimpanan objek \texttt{double} sebanyak 2 kali lipat dari data asli, yang membutuhkan memori sebesar 8 bytes untuk setiap objeknya. Sehingga menyebabkan limit memori yang diperlukan melebihi kapasitas memori yang dimiliki oleh \textit{node} pekerja.
	\end{enumerate}

	Berdasarkan hasil eksperimen juga didapat bahwa perangkat lunak yang dibangun memiliki waktu eksekusi cukup cepat untuk menangani perhitungan dengan atribut prediktor bertipe diskrit. Tetapi, untuk atribut prediktor bertipe numerik, perangkat lunak yang dibangun belum memiliki waktu eksekusi yang cukup cepat, dikarenakan hanya mampu menjalankan program dengan data yang maksimal berukuran sebesar 1.11GB saja pada spesifikasi perangkat keras yang sudah dijelaskan pada Subbab~\ref{sec:desc_perangkat}.
	
\end{itemize}

\section{Saran Penelitian Lanjutan}

Penelitian pada skripsi kali ini hanya merupakan sebuah langkah kecil dalam memanfaatkan teknik penambangan data berbasis \textit{MapReduce} pada sistem terdistribusi Hadoop. Penelitian selanjutnya diharapkan dapat melakukan lebih banyak lagi penerapan teknik/algoritma penambangan data dan/atau \textit{machine learning} pada sistem terdistribusi hadoop mengguanakan pemrograman berbasis \textit{MapReduce}. Selain itu, dapat juga dilakukan pemahaman lebih dalam mengenai internal desain dari sistem Hadoop untuk dapat memanfaatkan sumber daya dari \textit{cluster} yang kita miliki dengan maksimal. Seperti misalnya, menganalisis jumlah \textit{mapper} dan \textit{reducer} yang optimal untuk digunakan pada suatu proses \textit{MapReduce} untuk memaksimalkan penggunaan sumber daya dan meningkatkan efisiensi waktu pada pemrosesan data berukuran besar yang kita miliki.

Adapun penelitian yang dapat dilanjutkan jika mengacu pada penelitian ini adalah sebagai berikut:
\begin{itemize}
	\item Dapat dilakukan penambahan teknik penanganan untuk atribut bertipe numerik dengan menghitung nilai standard deviasi menggunakan pendekatan aproksimasi, dimana nilai standard deviasi didapat pada iterasi sebelumnya yang mengakibatkan nilai tersebut tidak mutlak kebenarannya tetapi dapat membantu perangkat lunak dalam mempercepat waktu eksekusi.
	\item Dapat dilakukan penambahan teknik penanganan untuk atribut bertipe numerik dengan memodifikasi alur kerja dari framework \textit{MapReduce} pada \textit{Hadoop} untuk mengulangi fase reduce setelah perhitungan rata - rata didapat pada fase reduce sebelumnya menggunakan \texttt{Chain Mapper}\footnote{\textit{Chain Mapper} merupakan fitur dari \textit{MapReduce} untuk menggunakan lebih dari satu kelas mapper pada satu pekerjaan. Sumber: \url{http://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapreduce/lib/chain/ChainMapper.html}}
	\item Dapat dilakukan penambahan untuk menerapkan teknik \textit{predictive regression} pada perangkat lunak yang sudah dibangun untuk melakukan prediksi terhadap atribut numerik.
	\item Dapat dilakukan pengembangan untuk dapat melakukan eksekusi program berbasis \textit{MapReduce} pada sistem terdistribusi hadoop menggunakan GUI (\textit{Graphical User Interface}) yang telah dibuat untuk modul kelola input dan klasifikasi. Sehingga, akan memudahkan \textit{end-user}(misalnya: pegawai pada perusahaan) untuk menjalankan program berbasis \textit{MapReduce}.
	\item Dapat dilakukan penambahan untuk menerapkan beberapa teknik penanganan atribut bertipe numerik seperti metode binning dsb.
	\item Melakukan eksperimen pada cluster yang lebih besar yang dapat mengikutsertakan lebih banyak node dari saat ini.
	\item Menganalisis variabel - variabel pada Hadoop yang dapat dikelola secara manual untuk mengoptimalkan kinerja dari program \textit{MapReduce} yang berjalan.
\end{itemize}

